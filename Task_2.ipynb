{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Remove all the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set env CUDA_LAUNCH_BLOCKING=1\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Retina display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "try:\n",
    "    from einops import rearrange\n",
    "except ImportError:\n",
    "    %pip install einops\n",
    "    from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('dog.jpg'):\n",
    "    print('dog.jpg exists')\n",
    "else:\n",
    "    !wget https://segment-anything.com/assets/gallery/AdobeStock_94274587_welsh_corgi_pembroke_CD.jpg -O dog.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a image from torchvision\n",
    "img = torchvision.io.read_image(\"dog.jpg\")\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rearrange(img, 'c h w -> h w c').numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler_img = preprocessing.MinMaxScaler().fit(img.reshape(-1, 1))\n",
    "scaler_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_scaled = scaler_img.transform(img.reshape(-1, 1)).reshape(img.shape)\n",
    "img_scaled.shape\n",
    "\n",
    "img_scaled = torch.tensor(img_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_scaled = img_scaled.to(device)\n",
    "img_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop = torchvision.transforms.functional.crop(img_scaled.cpu(), 600, 800, 300, 300)\n",
    "crop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rearrange(crop, 'c h w -> h w c').cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop = crop.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dimensions of the image tensor\n",
    "num_channels, height, width = crop.shape\n",
    "print(num_channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels, height, width = 2, 3, 4\n",
    "\n",
    "    \n",
    "# Create a 2D grid of (x,y) coordinates\n",
    "w_coords = torch.arange(width).repeat(height, 1)\n",
    "h_coords = torch.arange(height).repeat(width, 1).t()\n",
    "w_coords = w_coords.reshape(-1)\n",
    "h_coords = h_coords.reshape(-1)\n",
    "\n",
    "# Combine the x and y coordinates into a single tensor\n",
    "X = torch.stack([h_coords, w_coords], dim=1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coordinate_map(img):\n",
    "    \"\"\"\n",
    "    img: torch.Tensor of shape (num_channels, height, width)\n",
    "    \n",
    "    return: tuple of torch.Tensor of shape (height * width, 2) and torch.Tensor of shape (height * width, num_channels)\n",
    "    \"\"\"\n",
    "    \n",
    "    num_channels, height, width = img.shape\n",
    "    \n",
    "    # Create a 2D grid of (x,y) coordinates (h, w)\n",
    "    # width values change faster than height values\n",
    "    w_coords = torch.arange(width).repeat(height, 1)\n",
    "    h_coords = torch.arange(height).repeat(width, 1).t()\n",
    "    w_coords = w_coords.reshape(-1)\n",
    "    h_coords = h_coords.reshape(-1)\n",
    "\n",
    "    # Combine the x and y coordinates into a single tensor\n",
    "    X = torch.stack([h_coords, w_coords], dim=1).float()\n",
    "\n",
    "    # Move X to GPU if available\n",
    "    X = X.to(device)\n",
    "\n",
    "    # Reshape the image to (h * w, num_channels)\n",
    "    Y = rearrange(img, 'c h w -> (h w) c').float()\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_X, dog_Y = create_coordinate_map(crop)\n",
    "\n",
    "dog_X.shape, dog_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaler from -1 to 1\n",
    "scaler_X = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(dog_X.cpu())\n",
    "\n",
    "# Scale the X coordinates\n",
    "dog_X_scaled = scaler_X.transform(dog_X.cpu())\n",
    "\n",
    "# Move the scaled X coordinates to the GPU\n",
    "dog_X_scaled = torch.tensor(dog_X_scaled).to(device)\n",
    "\n",
    "# Set to dtype float32\n",
    "dog_X_scaled = dog_X_scaled.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, lr, X, Y, epochs, verbose=True):\n",
    "    \"\"\"\n",
    "    net: torch.nn.Module\n",
    "    lr: float\n",
    "    X: torch.Tensor of shape (num_samples, 2)\n",
    "    Y: torch.Tensor of shape (num_samples, 3)\n",
    "    \"\"\"\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(X)\n",
    "        \n",
    "        \n",
    "        loss = criterion(outputs, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if verbose and epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch} loss: {loss.item():.6f}\")\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RFF features\n",
    "def create_rff_features(X, num_features, sigma):\n",
    "    from sklearn.kernel_approximation import RBFSampler\n",
    "    rff = RBFSampler(n_components=num_features, gamma=1/(2 * sigma**2))\n",
    "    X = X.cpu().numpy()\n",
    "    X = rff.fit_transform(X)\n",
    "    return torch.tensor(X, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rff = create_rff_features(dog_X_scaled, 37500, 0.008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LinearModel(X_rff.shape[1], 3)\n",
    "net.to(device)\n",
    "\n",
    "train(net, 0.005, X_rff, dog_Y, 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstructed_and_original_image(original_img, net, X, title=\"\"):\n",
    "    \"\"\"\n",
    "    net: torch.nn.Module\n",
    "    X: torch.Tensor of shape (num_samples, 2)\n",
    "    Y: torch.Tensor of shape (num_samples, 3)\n",
    "    \"\"\"\n",
    "    num_channels, height, width = original_img.shape\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = net(X)\n",
    "        outputs = outputs.reshape(height, width, num_channels)\n",
    "        #outputs = outputs.permute(1, 2, 0)\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])\n",
    "\n",
    "    ax0 = plt.subplot(gs[0])\n",
    "    ax1 = plt.subplot(gs[1])\n",
    "\n",
    "    ax0.imshow(outputs.cpu())\n",
    "    ax0.set_title(\"Reconstructed Image\")\n",
    "    \n",
    "\n",
    "    ax1.imshow(original_img.cpu().permute(1, 2, 0))\n",
    "    ax1.set_title(\"Original Image\")\n",
    "    \n",
    "    for a in [ax0, ax1]:\n",
    "        a.axis(\"off\")\n",
    "\n",
    "\n",
    "    fig.suptitle(title, y=0.9)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstructed_and_original_image(crop, net, X_rff, title=\"Reconstructed Image with RFF Features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
