{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AomtBF2P2E2h"
      },
      "source": [
        "Image Reconstruction\n",
        "\n",
        "Choose any image you like. Use Random Fourier Features (RFF) and Linear Regression to learn the mapping from the image coordinates (X, Y) to the pixel colors (R, G, B). Here, (X, Y) represents the coordinates of the pixels, and (R, G, B) represents the color values at those coordinates.\n",
        "\n",
        "1. **Load Image**: Select any image of your choice.\n",
        "2. **Random Fourier Features (RFF)**: Implement RFF to map pixel coordinates to color values.\n",
        "3. **Linear Regression**: Use linear regression to learn the mapping.\n",
        "4. **Display Results**: Show both the original and reconstructed images.\n",
        "5. **Metrics**: Calculate the Root Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR) between the original and reconstructed images.\n",
        "\n",
        "**Key Variables**:\n",
        "- X, Y: Pixel coordinates.\n",
        "- R, G, B: Pixel color values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng6UtYTC2E2i"
      },
      "source": [
        "#### Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-6NzlyK2E2i",
        "outputId": "1f2a2c8f-1b83-4f35-c10f-51f50fb00444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Remove all the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set env CUDA_LAUNCH_BLOCKING=1\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Retina display\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "try:\n",
        "    from einops import rearrange\n",
        "except ImportError:\n",
        "    %pip install einops\n",
        "\n",
        "    from einops import rearrange"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.get_device_name(0))\n",
        "print(\"CUDA available:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "QVq4V7Qo3bnm",
        "outputId": "9719c186-4f52-4300-a6fd-bdf47433cc31"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-c40f4421b9e3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CUDA available:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[0;32m--> 414\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \"\"\"\n\u001b[0;32m--> 444\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tehY3M7g2E2i"
      },
      "source": [
        "#### Importing image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uMUqUX32E2j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define the directory path\n",
        "directory_path = '../assets/images/'\n",
        "\n",
        "# Check if the directory exists, and create it if it does not\n",
        "if not os.path.exists(directory_path):\n",
        "    os.makedirs(directory_path)\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(os.path.join(directory_path, 'dog.jpg')):\n",
        "    print('dog.jpg exists')\n",
        "else:\n",
        "    # Download the file if it does not exist\n",
        "    !wget https://segment-anything.com/assets/gallery/AdobeStock_94274587_welsh_corgi_pembroke_CD.jpg -O ../assets/images/dog.jpg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfpLsvGv2E2j"
      },
      "outputs": [],
      "source": [
        "# Read in a image from torchvision\n",
        "img = torchvision.io.read_image(\"../assets/images/dog.jpg\")\n",
        "print(img.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg0V2vQS2E2j"
      },
      "outputs": [],
      "source": [
        "plt.imshow(rearrange(img, 'c h w -> h w c').numpy())\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDbkONTy2E2j"
      },
      "outputs": [],
      "source": [
        "# # Read in a image from torchvision\n",
        "# iitgn_img = torchvision.io.read_image(\"../assets/images/iitgn.jpg\")\n",
        "# print(iitgn_img.shape)\n",
        "# plt.imshow(rearrange(iitgn_img, 'c h w -> h w c').numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqF-dvdV2E2j"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "scaler_img = preprocessing.MinMaxScaler().fit(img.reshape(-1, 1))\n",
        "scaler_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Njg6Vs1j2E2k"
      },
      "outputs": [],
      "source": [
        "\n",
        "img_scaled = scaler_img.transform(img.reshape(-1, 1)).reshape(img.shape)\n",
        "print(img_scaled.shape)\n",
        "\n",
        "img_scaled = torch.tensor(img_scaled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kAV6vp82E2k"
      },
      "outputs": [],
      "source": [
        "img_scaled = img_scaled.to(device)\n",
        "img_scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0yrB0-k2E2l"
      },
      "source": [
        "#### Crop the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeE4dqtz2E2l"
      },
      "outputs": [],
      "source": [
        "crop = torchvision.transforms.functional.crop(img_scaled.cpu(), 600, 800, 300, 300)\n",
        "crop.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyT5Q7rY2E2l"
      },
      "outputs": [],
      "source": [
        "plt.imshow(rearrange(crop, 'c h w -> h w c').cpu().numpy())\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnTfz3ib2E2l"
      },
      "outputs": [],
      "source": [
        "crop = crop.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC0JpjIX2E2l"
      },
      "source": [
        "#### Create a coordinate map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_4v6UGh2E2l"
      },
      "outputs": [],
      "source": [
        "def create_coordinate_map(img):\n",
        "    \"\"\"\n",
        "    img: torch.Tensor of shape (num_channels, height, width)\n",
        "\n",
        "    return: tuple of torch.Tensor of shape (height * width, 2) and torch.Tensor of shape (height * width, num_channels)\n",
        "    \"\"\"\n",
        "\n",
        "    num_channels, height, width = img.shape\n",
        "    print(\"Number of channels:\", num_channels, \"\\nHeight:\", height, \"\\nWidth:\", width)\n",
        "    # Create a 2D grid of (x,y) coordinates (h, w)\n",
        "    # width values change faster than height values\n",
        "    w_coords = torch.arange(width).repeat(height, 1)\n",
        "    h_coords = torch.arange(height).repeat(width, 1).t()\n",
        "    w_coords = w_coords.reshape(-1)\n",
        "    h_coords = h_coords.reshape(-1)\n",
        "\n",
        "    # Combine the x and y coordinates into a single tensor\n",
        "    X = torch.stack([h_coords, w_coords], dim=1).float()\n",
        "\n",
        "    # Move X to GPU if available\n",
        "    X = X.to(device)\n",
        "    print(\"X shape:\", X.shape)\n",
        "    # Reshape the image to (h * w, num_channels)\n",
        "    Y = rearrange(img, 'c h w -> (h w) c').float()\n",
        "    print(\"Y shape:\", Y.shape)\n",
        "\n",
        "    return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0-n7P6z2E2m"
      },
      "outputs": [],
      "source": [
        "dog_X, dog_Y = create_coordinate_map(crop)\n",
        "\n",
        "print(dog_X) # (300*300, 2)- coordinates\n",
        "print(dog_Y) # (300*300, 3)- RGB values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzdHW9ME2E2m"
      },
      "outputs": [],
      "source": [
        "# MinMaxScaler from -1 to 1\n",
        "scaler_X = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(dog_X.cpu())\n",
        "\n",
        "# Scale the X coordinates\n",
        "dog_X_scaled = scaler_X.transform(dog_X.cpu())\n",
        "\n",
        "# Move the scaled X coordinates to the GPU\n",
        "dog_X_scaled = torch.tensor(dog_X_scaled).to(device)\n",
        "\n",
        "# Set to dtype float32\n",
        "dog_X_scaled = dog_X_scaled.float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRrUUUMC2E2m"
      },
      "source": [
        "#### Reconstructing using Linear Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYAVdZur2E2m"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42) # Set seed for reproducibility\n",
        "\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4jikg1R2E2m"
      },
      "outputs": [],
      "source": [
        "net = LinearModel(2, 3)\n",
        "net.to(device)\n",
        "print(net)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lruWBsPA2E2m"
      },
      "outputs": [],
      "source": [
        "print(\"Weights:\", net.linear.weight)\n",
        "print(\"Bias:\", net.linear.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALWXwzdz2E2m"
      },
      "outputs": [],
      "source": [
        "def train(net, lr, X, Y, epochs, verbose=True):\n",
        "    \"\"\"\n",
        "    net: torch.nn.Module\n",
        "    lr: float\n",
        "    X: torch.Tensor of shape (num_samples, 2)\n",
        "    Y: torch.Tensor of shape (num_samples, 3)\n",
        "    \"\"\"\n",
        "    losses = []\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    for epoch in range(1,epochs+1):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(X)\n",
        "        loss = criterion(outputs, Y)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if verbose and epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch} loss: {loss.item():.6f}\")\n",
        "    return loss.item(),losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0Wbsb2A2E2m"
      },
      "outputs": [],
      "source": [
        "train_loss,training_losses = train(net, 0.01, dog_X_scaled, dog_Y, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from latexify import latexify , format_axes\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "tkgn7705_XnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPVfOlPl2E2m"
      },
      "outputs": [],
      "source": [
        "# plot training loss graph\n",
        "plt.figure(figsize=(10, 5))\n",
        "latexify()\n",
        "format_axes(plt.gca())\n",
        "plt.plot(training_losses)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GBnrJ472E2m"
      },
      "outputs": [],
      "source": [
        "def plot_reconstructed_and_original_image(original_img, net, X, title=\"\"):\n",
        "    \"\"\"\n",
        "    net: torch.nn.Module\n",
        "    X: torch.Tensor of shape (num_samples, 2)\n",
        "    Y: torch.Tensor of shape (num_samples, 3)\n",
        "    \"\"\"\n",
        "    num_channels, height, width = original_img.shape\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = net(X)\n",
        "        outputs = outputs.reshape(height, width, num_channels)\n",
        "        #outputs = outputs.permute(1, 2, 0)\n",
        "    fig = plt.figure(figsize=(6, 4))\n",
        "    gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])\n",
        "\n",
        "    ax0 = plt.subplot(gs[0])\n",
        "    ax1 = plt.subplot(gs[1])\n",
        "\n",
        "    ax0.imshow(outputs.cpu())\n",
        "    ax0.set_title(\"Reconstructed Image\")\n",
        "\n",
        "\n",
        "    ax1.imshow(original_img.cpu().permute(1, 2, 0))\n",
        "    ax1.set_title(\"Original Image\")\n",
        "\n",
        "    for a in [ax0, ax1]:\n",
        "        a.axis(\"off\")\n",
        "\n",
        "\n",
        "    fig.suptitle(title, y=0.9)\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Kau8IBw2E2n"
      },
      "outputs": [],
      "source": [
        "plot_reconstructed_and_original_image(crop, net, dog_X_scaled, title=\"Reconstructed Image\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4cdYYz72E2n"
      },
      "source": [
        "#### Using Polynomial Basis Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAfs68wg2E2n"
      },
      "outputs": [],
      "source": [
        "# Use polynomial features of degree \"d\"\n",
        "\n",
        "def poly_features(X, degree):\n",
        "    \"\"\"\n",
        "    X: torch.Tensor of shape (num_samples, 2)\n",
        "    degree: int\n",
        "\n",
        "    return: torch.Tensor of shape (num_samples, degree * (degree + 1) / 2)\n",
        "    \"\"\"\n",
        "    X1 = X[:, 0]\n",
        "    X2 = X[:, 1]\n",
        "    X1 = X1.unsqueeze(1)\n",
        "    X2 = X2.unsqueeze(1)\n",
        "    X = torch.cat([X1, X2], dim=1)\n",
        "    poly = preprocessing.PolynomialFeatures(degree=degree)\n",
        "    X = poly.fit_transform(X.cpu())\n",
        "    return torch.tensor(X, dtype=torch.float32).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_image(image):\n",
        "    \"\"\"Normalize image to [0, 1] range for float images.\"\"\"\n",
        "    image_min = image.min()\n",
        "    image_max = image.max()\n",
        "    return (image - image_min) / (image_max - image_min)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + torch.exp(-x))\n",
        "\n",
        "def apply_sigmoid(image_tensor):\n",
        "    \"\"\"Apply sigmoid to image tensor to normalize it to [0, 1] range.\"\"\"\n",
        "    return sigmoid(image_tensor)\n",
        "\n",
        "def clip(image_tensor):\n",
        "    \"\"\"Clip image tensor to [0, 1] range.\"\"\"\n",
        "    return torch.clamp(image_tensor, 0, 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "U0-TjSDo799p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iyp7tyk2E2n"
      },
      "outputs": [],
      "source": [
        "# Define polynomial degrees to test\n",
        "degrees = [5, 10, 50, 100]\n",
        "\n",
        "# Initialize lists to store training losses and images for each degree\n",
        "training_losses = []\n",
        "original_images = []\n",
        "reconstructed_clipped_images = []\n",
        "\n",
        "for degree in degrees:\n",
        "    # Create polynomial features\n",
        "    dog_X_scaled_poly = poly_features(dog_X_scaled, degree)\n",
        "    print(f\"Degree {degree}: {dog_X_scaled_poly.dtype}, {dog_X_scaled_poly.shape}, {dog_Y.shape}, {dog_Y.dtype}\")\n",
        "\n",
        "    # Initialize and train the model\n",
        "    net = LinearModel(dog_X_scaled_poly.shape[1], 3)\n",
        "    net.to(device)\n",
        "\n",
        "    # Train the model\n",
        "    train_poly_loss, losses = train(net, 0.005, dog_X_scaled_poly, dog_Y, 1500)\n",
        "    training_losses.append(losses)\n",
        "\n",
        "    # Generate reconstructed image\n",
        "    with torch.no_grad():\n",
        "        output = net(dog_X_scaled_poly)\n",
        "\n",
        "    # Reshape output and apply transformations\n",
        "    reconstructed_image = output.cpu().reshape(crop.shape[1], crop.shape[2], -1)\n",
        "    clipped_image = clip(reconstructed_image)\n",
        "\n",
        "    # Append images to lists with correct shape\n",
        "    original_images.append(rearrange(crop, 'c h w -> h w c').cpu().numpy())\n",
        "    reconstructed_clipped_images.append(clipped_image.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XVAYdeiJZir-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(original_images[0].shape)\n",
        "print(reconstructed_clipped_images[0].shape)"
      ],
      "metadata": {
        "id": "TIE2e0zW_-mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reconstructed_clipped_images[0][0,:5,:])"
      ],
      "metadata": {
        "id": "-XrpUWWgGRQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure with subplots for loss and images\n",
        "fig, axs = plt.subplots(len(degrees),3, figsize=(10, len(degrees) * 3))\n",
        "\n",
        "latexify()\n",
        "\n",
        "\n",
        "for i, degree in enumerate(degrees):\n",
        "    # Plot training loss\n",
        "    axs[i, 0].plot(training_losses[i])\n",
        "    axs[i, 0].set_title(f\"Training Loss for Degree {degree}\")\n",
        "    axs[i, 0].set_xlabel(\"Epoch\")\n",
        "    axs[i, 0].set_ylabel(\"Loss\")\n",
        "    format_axes(axs[i, 0])\n",
        "\n",
        "    # Plot original image\n",
        "    axs[i, 1].imshow(original_images[0])\n",
        "    axs[i, 1].set_title(f\"Original Image\")\n",
        "    axs[i, 1].axis('off')\n",
        "\n",
        "    # Plot reconstructed clipped image\n",
        "    axs[i, 2].imshow(reconstructed_clipped_images[i])\n",
        "    axs[i, 2].set_title(f\"Reconstructed Clipped Image (Degree {degree})\")\n",
        "    axs[i, 2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fuTl3hXx6Mxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot all training losses in a single plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, degree in enumerate(degrees):\n",
        "    plt.plot(training_losses[i], label=f'Degree {degree}')\n",
        "plt.title(\"Training Loss for Different Polynomial Degrees\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T-6zd4it8_3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reconstructing using Random Fourier Features"
      ],
      "metadata": {
        "id": "7JWbm_DaZjuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create RFF features\n",
        "def create_rff_features(X, num_features, sigma):\n",
        "    from sklearn.kernel_approximation import RBFSampler\n",
        "    rff = RBFSampler(n_components=num_features, gamma=1/(2 * sigma**2))\n",
        "    X = X.cpu().numpy()\n",
        "    X = rff.fit_transform(X)\n",
        "    return torch.tensor(X, dtype=torch.float32).to(device)\n"
      ],
      "metadata": {
        "id": "BnItZWyRZpbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = 37500\n",
        "sigma = 0.008\n",
        "X_rff = create_rff_features(dog_X_scaled, n_features, sigma)\n",
        "print(X_rff.shape)"
      ],
      "metadata": {
        "id": "-CE4s42UZnEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = LinearModel(X_rff.shape[1], 3)\n",
        "net.to(device)\n",
        "\n",
        "train_rff_loss , train_rff_losses = train(net, 0.005, X_rff, dog_Y, 2500)"
      ],
      "metadata": {
        "id": "OFsbK_nDaMGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot training loss graph\n",
        "plt.figure(figsize=(10, 5))\n",
        "latexify()\n",
        "format_axes(plt.gca())\n",
        "plt.plot(train_rff_losses)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "x6vTadLYaYOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_reconstructed_and_original_image(crop, net, X_rff, title=\"Reconstructed Image with RFF Features\")"
      ],
      "metadata": {
        "id": "Js3E6x-hZ-Jz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}