{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"V28"},"accelerator":"TPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9392721,"sourceType":"datasetVersion","datasetId":5700164}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Image Reconstruction\n\nChoose any image you like. Use Random Fourier Features (RFF) and Linear Regression to learn the mapping from the image coordinates (X, Y) to the pixel colors (R, G, B). Here, (X, Y) represents the coordinates of the pixels, and (R, G, B) represents the color values at those coordinates.\n\n1. **Load Image**: Select any image of your choice.\n2. **Random Fourier Features (RFF)**: Implement RFF to map pixel coordinates to color values.\n3. **Linear Regression**: Use linear regression to learn the mapping.\n4. **Display Results**: Show both the original and reconstructed images.\n5. **Metrics**: Calculate the Root Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR) between the original and reconstructed images.\n\n**Key Variables**:\n- X, Y: Pixel coordinates.\n- R, G, B: Pixel color values.\n","metadata":{"id":"AomtBF2P2E2h"}},{"cell_type":"markdown","source":"### Latexify","metadata":{}},{"cell_type":"code","source":"'''\nCode snippet copied from [https://github.com/nipunbatra/ml-teaching/blob/e2cd59d3e3358473ebfcf70e71d70361bb4501b4/latexify.py#L9] by Nipun Batra\nDate: 14 th August , 2024\n\nChanges :\n'text.latex.preamble': '\\\\usepackage{gensymb}',\n'text.usetex': False,\n\n'''\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport matplotlib\n\nfrom math import sqrt\nSPINE_COLOR = 'gray'\n\ndef latexify(fig_width=None, fig_height=None, columns=1):\n    \"\"\"Set up matplotlib's RC params for LaTeX plotting.\n    Call this before plotting a figure.\n\n    Parameters\n    ----------\n    fig_width : float, optional, inches\n    fig_height : float,  optional, inches\n    columns : {1, 2}\n    \"\"\"\n\n    # code adapted from http://www.scipy.org/Cookbook/Matplotlib/LaTeX_Examples\n\n    # Width and max height in inches for IEEE journals taken from\n    # computer.org/cms/Computer.org/Journal%20templates/transactions_art_guide.pdf\n\n    assert(columns in [1,2])\n\n    if fig_width is None:\n        fig_width = 3.39 if columns==1 else 6.9 # width in inches\n\n    if fig_height is None:\n        golden_mean = (sqrt(5)-1.0)/2.0    # Aesthetic ratio\n        fig_height = fig_width*golden_mean # height in inches\n\n    MAX_HEIGHT_INCHES = 8.0\n    if fig_height > MAX_HEIGHT_INCHES:\n        print(\"WARNING: fig_height too large:\" + fig_height + \n              \"so will reduce to\" + MAX_HEIGHT_INCHES + \"inches.\")\n        fig_height = MAX_HEIGHT_INCHES\n\n    params = {'backend': 'ps',\n              'text.latex.preamble': '\\\\usepackage{gensymb}',\n              'axes.labelsize': 8, # fontsize for x and y labels (was 10)\n              'axes.titlesize': 8,\n              'font.size': 8, # was 10\n              'legend.fontsize': 8, # was 10\n              'xtick.labelsize': 8,\n              'ytick.labelsize': 8,\n              'text.usetex': False,\n              'figure.figsize': [fig_width,fig_height],\n              'font.family': 'serif'\n    }\n\n    matplotlib.rcParams.update(params)\n\n\ndef format_axes(ax):\n\n    for spine in ['top', 'right']:\n        ax.spines[spine].set_visible(False)\n\n    for spine in ['left', 'bottom']:\n        ax.spines[spine].set_color(SPINE_COLOR)\n        ax.spines[spine].set_linewidth(0.5)\n\n    ax.xaxis.set_ticks_position('bottom')\n    ax.yaxis.set_ticks_position('left')\n\n    for axis in [ax.xaxis, ax.yaxis]:\n        axis.set_tick_params(direction='out', color=SPINE_COLOR)\n\n    return ax\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing necessary libraries","metadata":{"id":"Ng6UtYTC2E2i"}},{"cell_type":"code","source":"import torch\nprint(torch.__version__)\n\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Remove all the warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set env CUDA_LAUNCH_BLOCKING=1\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\ntry:\n    from einops import rearrange\nexcept ImportError:\n    %pip install einops\n\n    from einops import rearrange","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L-6NzlyK2E2i","outputId":"1f2a2c8f-1b83-4f35-c10f-51f50fb00444","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(torch.cuda.get_device_name(0))\nprint(\"CUDA available:\", torch.cuda.is_available())","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":335},"id":"QVq4V7Qo3bnm","outputId":"9719c186-4f52-4300-a6fd-bdf47433cc31","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing image","metadata":{"id":"tehY3M7g2E2i"}},{"cell_type":"code","source":"import os\n\n# Define the directory path\ndirectory_path = '../assets/images/'\n\n# Check if the directory exists, and create it if it does not\nif not os.path.exists(directory_path):\n    os.makedirs(directory_path)\n\n# Check if the file exists\nif os.path.exists(os.path.join(directory_path, 'dog.jpg')):\n    print('dog.jpg exists')\nelse:\n    # Download the file if it does not exist\n    !wget https://segment-anything.com/assets/gallery/AdobeStock_94274587_welsh_corgi_pembroke_CD.jpg -O ../assets/images/dog.jpg\n","metadata":{"id":"3uMUqUX32E2j","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read in a image from torchvision\nimg = torchvision.io.read_image(\"../assets/images/dog.jpg\")\nprint(img.shape)","metadata":{"id":"KfpLsvGv2E2j","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(rearrange(img, 'c h w -> h w c').numpy())\nplt.axis('off')\nplt.show()","metadata":{"id":"gg0V2vQS2E2j","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Read in a image from torchvision\n# iitgn_img = torchvision.io.read_image(\"../assets/images/iitgn.jpg\")\n# print(iitgn_img.shape)\n# plt.imshow(rearrange(iitgn_img, 'c h w -> h w c').numpy())\n","metadata":{"id":"xDbkONTy2E2j","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\n\nscaler_img = preprocessing.MinMaxScaler().fit(img.reshape(-1, 1))\nscaler_img","metadata":{"id":"KqF-dvdV2E2j","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimg_scaled = scaler_img.transform(img.reshape(-1, 1)).reshape(img.shape)\nprint(img_scaled.shape)\n\nimg_scaled = torch.tensor(img_scaled)\n","metadata":{"id":"Njg6Vs1j2E2k","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_scaled = img_scaled.to(device)\nimg_scaled","metadata":{"id":"1kAV6vp82E2k","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Crop the image","metadata":{"id":"G0yrB0-k2E2l"}},{"cell_type":"code","source":"crop = torchvision.transforms.functional.crop(img_scaled.cpu(), 600, 800, 300, 300)\ncrop.shape","metadata":{"id":"yeE4dqtz2E2l","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(rearrange(crop, 'c h w -> h w c').cpu().numpy())\nplt.axis('off')\nplt.show()","metadata":{"id":"kyT5Q7rY2E2l","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"crop = crop.to(device)","metadata":{"id":"SnTfz3ib2E2l","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a coordinate map","metadata":{"id":"CC0JpjIX2E2l"}},{"cell_type":"code","source":"def create_coordinate_map(img):\n    \"\"\"\n    img: torch.Tensor of shape (num_channels, height, width)\n\n    return: tuple of torch.Tensor of shape (height * width, 2) and torch.Tensor of shape (height * width, num_channels)\n    \"\"\"\n\n    num_channels, height, width = img.shape\n    print(\"Number of channels:\", num_channels, \"\\nHeight:\", height, \"\\nWidth:\", width)\n    # Create a 2D grid of (x,y) coordinates (h, w)\n    # width values change faster than height values\n    w_coords = torch.arange(width).repeat(height, 1)\n    h_coords = torch.arange(height).repeat(width, 1).t()\n    w_coords = w_coords.reshape(-1)\n    h_coords = h_coords.reshape(-1)\n\n    # Combine the x and y coordinates into a single tensor\n    X = torch.stack([h_coords, w_coords], dim=1).float()\n\n    # Move X to GPU if available\n    X = X.to(device)\n    print(\"X shape:\", X.shape)\n    # Reshape the image to (h * w, num_channels)\n    Y = rearrange(img, 'c h w -> (h w) c').float()\n    print(\"Y shape:\", Y.shape)\n\n    return X, Y","metadata":{"id":"1_4v6UGh2E2l","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dog_X, dog_Y = create_coordinate_map(crop)\n\nprint(dog_X) # (300*300, 2)- coordinates\nprint(dog_Y) # (300*300, 3)- RGB values","metadata":{"id":"l0-n7P6z2E2m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MinMaxScaler from -1 to 1\nscaler_X = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(dog_X.cpu())\n\n# Scale the X coordinates\ndog_X_scaled = scaler_X.transform(dog_X.cpu())\n\n# Move the scaled X coordinates to the GPU\ndog_X_scaled = torch.tensor(dog_X_scaled).to(device)\n\n# Set to dtype float32\ndog_X_scaled = dog_X_scaled.float()","metadata":{"id":"pzdHW9ME2E2m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Functions to Calculate RMSE and PSNR","metadata":{}},{"cell_type":"code","source":"# Functions to calculate RMSE and PSNR\ndef calculate_rmse(original_image, reconstructed_image):\n    \"\"\"Calculate the RMSE between the original and reconstructed images.\"\"\"\n    mse = torch.mean((original_image - reconstructed_image) ** 2)\n    rmse = torch.sqrt(mse)\n    return rmse.item()\n\ndef calculate_psnr(original_image, reconstructed_image, max_pixel_value=1.0):\n    \"\"\"Calculate the PSNR between the original and reconstructed images.\"\"\"\n    mse = torch.mean((original_image - reconstructed_image) ** 2)\n    if mse == 0:  # MSE is zero means images are identical\n        return float('inf')\n    psnr = 20 * torch.log10(max_pixel_value / torch.sqrt(mse))\n    return psnr.item()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setting up device","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.cuda.amp import autocast, GradScaler\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\n# Set the random seed for reproducibility\ntorch.manual_seed(42)\n\n# Detect GPU and enable multi-GPU usage\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Defining Linear Model","metadata":{}},{"cell_type":"code","source":"# Define the linear model\nclass LinearModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LinearModel, self).__init__()\n        self.fc = nn.Linear(input_dim, output_dim)\n    \n    def forward(self, x):\n        return self.fc(x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reconstructing using Linear Model","metadata":{"id":"wRrUUUMC2E2m"}},{"cell_type":"code","source":"# Instantiate and move the model to the device (support multiple GPUs)\nnet = LinearModel(2, 3).to(device)\nif torch.cuda.device_count() > 1:\n    net = nn.DataParallel(net)\n\nprint(net)\nprint(\"Weights:\", net.module.linear.weight if torch.cuda.device_count() > 1 else net.linear.weight)\nprint(\"Bias:\", net.module.linear.bias if torch.cuda.device_count() > 1 else net.linear.bias)\n\n# Training function with mixed precision and memory optimization\ndef train(net, lr, X, Y, epochs, batch_size=512, verbose=True):\n    \"\"\"\n    net: torch.nn.Module\n    lr: float\n    X: torch.Tensor of shape (num_samples, 2)\n    Y: torch.Tensor of shape (num_samples, 3)\n    \"\"\"\n    losses = []\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n    scaler = GradScaler()  # Use mixed precision training for efficiency\n\n    # DataLoader to handle batch processing and shuffle the data\n    dataset = TensorDataset(X, Y)\n    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    for epoch in range(1, epochs+1):\n        epoch_loss = 0.0\n        for batch_X, batch_Y in data_loader:\n            batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)\n\n            optimizer.zero_grad()\n\n            # Mixed precision training\n            with autocast():\n                outputs = net(batch_X)\n                loss = criterion(outputs, batch_Y)\n\n            # Scale loss and backpropagate\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            epoch_loss += loss.item()\n\n        # Track loss per epoch\n        losses.append(epoch_loss / len(data_loader))\n\n        if verbose and epoch % 200 == 0:\n            print(f\"Epoch {epoch} loss: {epoch_loss / len(data_loader):.6f}\")\n\n        # Clear unused memory after each epoch\n        torch.cuda.empty_cache()\n\n    return losses\n\n# Training on dog_X_scaled and dog_Y (Assuming dog_X_scaled and dog_Y are available)\ntrain_loss, training_losses = train(net, 0.01, dog_X_scaled, dog_Y, 1000)\n\n# Plot training loss graph\nplt.figure(figsize=(10, 5))\nplt.plot(training_losses)\nplt.title(\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.show()\n\n\n# Assuming original_images and reconstructed_clipped_images are available\noriginal_image = torch.tensor(original_images[-1]).to(device)\nreconstructed_clipped_image = torch.tensor(reconstructed_clipped_images[-1]).to(device)\n\n# Calculate RMSE and PSNR\nrmse_value = calculate_rmse(original_image, reconstructed_clipped_image)\nprint(f\"RMSE: {rmse_value}\")\n\npsnr_value = calculate_psnr(original_image, reconstructed_clipped_image)\nprint(f\"PSNR: {psnr_value}\")\n\n# Plot function for reconstructed and original images\ndef plot_reconstructed_and_original_image(original_img, net, X, title=\"\"):\n    \"\"\"\n    net: torch.nn.Module\n    X: torch.Tensor of shape (num_samples, 2)\n    Y: torch.Tensor of shape (num_samples, 3)\n    \"\"\"\n    net.eval()\n    with torch.no_grad():\n        outputs = net(X).cpu().numpy()\n\n    num_channels, height, width = original_img.shape\n    outputs = outputs.reshape(height, width, num_channels)\n\n    fig = plt.figure(figsize=(6, 4))\n    gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])\n\n    ax0 = plt.subplot(gs[0])\n    ax1 = plt.subplot(gs[1])\n\n    ax0.imshow(outputs)\n    ax0.set_title(\"Reconstructed Image\")\n\n    ax1.imshow(original_img.cpu().permute(1, 2, 0))\n    ax1.set_title(\"Original Image\")\n\n    for a in [ax0, ax1]:\n        a.axis(\"off\")\n\n    fig.suptitle(title, y=0.9)\n    plt.tight_layout()\n\n# Assuming crop and dog_X_scaled are available\nplot_reconstructed_and_original_image(crop, net, dog_X_scaled, title=\"Reconstructed Image\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using Polynomial Basis Functions","metadata":{"id":"A4cdYYz72E2n"}},{"cell_type":"code","source":"def normalize_image(image):\n    \"\"\"Normalize image to [0, 1] range for float images.\"\"\"\n    image_min = image.min()\n    image_max = image.max()\n    return (image - image_min) / (image_max - image_min)\n\ndef sigmoid(x):\n    return 1 / (1 + torch.exp(-x))\n\ndef apply_sigmoid(image_tensor):\n    \"\"\"Apply sigmoid to image tensor to normalize it to [0, 1] range.\"\"\"\n    return sigmoid(image_tensor)\n\ndef clip(image_tensor):\n    \"\"\"Clip image tensor to [0, 1] range.\"\"\"\n    return torch.clamp(image_tensor, 0, 1)\n\n","metadata":{"id":"U0-TjSDo799p","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Use polynomial features of degree \"d\"\n\n# def poly_features(X, degree):\n#     \"\"\"\n#     X: torch.Tensor of shape (num_samples, 2)\n#     degree: int\n\n#     return: torch.Tensor of shape (num_samples, degree * (degree + 1) / 2)\n#     \"\"\"\n#     X1 = X[:, 0]\n#     X2 = X[:, 1]\n#     X1 = X1.unsqueeze(1)\n#     X2 = X2.unsqueeze(1)\n#     X = torch.cat([X1, X2], dim=1)\n#     poly = preprocessing.PolynomialFeatures(degree=degree)\n#     X = poly.fit_transform(X.cpu())\n#     return torch.tensor(X, dtype=torch.float32).to(device)\n\n\n\n# # Define polynomial degrees to test\n# degrees = [5, 10, 50, 100]\n\n# # Initialize lists to store training losses and images for each degree\n# training_losses = []\n# original_images = []\n# reconstructed_clipped_images = []\n\n# for degree in degrees:\n#     # Create polynomial features\n#     dog_X_scaled_poly = poly_features(dog_X_scaled, degree)\n#     print(f\"Degree {degree}: {dog_X_scaled_poly.dtype}, {dog_X_scaled_poly.shape}, {dog_Y.shape}, {dog_Y.dtype}\")\n\n#     # Initialize and train the model\n#     net = LinearModel(dog_X_scaled_poly.shape[1], 3)\n#     net.to(device)\n\n#     # Train the model\n#     train_poly_loss, losses = train(net, 0.005, dog_X_scaled_poly, dog_Y, 1500)\n#     training_losses.append(losses)\n\n#     # Generate reconstructed image\n#     with torch.no_grad():\n#         output = net(dog_X_scaled_poly)\n\n#     # Reshape output and apply transformations\n#     reconstructed_image = output.cpu().reshape(crop.shape[1], crop.shape[2], -1)\n#     clipped_image = clip(reconstructed_image)\n\n#     # Append images to lists with correct shape\n#     original_images.append(rearrange(crop, 'c h w -> h w c').cpu().numpy())\n#     reconstructed_clipped_images.append(clipped_image.numpy())","metadata":{"id":"9iyp7tyk2E2n","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"ALWXwzdz2E2m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define polynomial degrees to test\ndegrees = [5, 10, 50, 100]\n\n\ndef poly_features(X, degree, batch_size=1000):\n    \"\"\"\n    X: torch.Tensor of shape (num_samples, 2)\n    degree: int\n    batch_size: int (split processing into smaller batches to reduce memory usage)\n    \n    return: torch.Tensor of shape (num_samples, degree * (degree + 1) / 2)\n    \"\"\"\n    poly = preprocessing.PolynomialFeatures(degree=degree)\n\n    # Process in batches to avoid excessive RAM usage\n    X_batches = []\n    for i in range(0, X.shape[0], batch_size):\n        X_batch = X[i:i+batch_size].cpu()  # Move to CPU for the transformation\n        X_poly = poly.fit_transform(X_batch)  # Use PolynomialFeatures\n        X_batches.append(torch.tensor(X_poly, dtype=torch.float32).to(device))\n\n    return torch.cat(X_batches, dim=0)  # Concatenate all batches\n\n# Adjusting the train loop to minimize memory usage\ndef run():\n\n    # Initialize lists to store training losses and images for each degree\n    training_losses = []\n    original_images = []\n    reconstructed_clipped_images = []\n\n    for degree in degrees:\n        # Create polynomial features\n        dog_X_scaled_poly = poly_features(dog_X_scaled, degree, batch_size=1000)  # Use batch processing\n        print(f\"Degree {degree}: {dog_X_scaled_poly.dtype}, {dog_X_scaled_poly.shape}, {dog_Y.shape}, {dog_Y.dtype}\")\n\n        # Initialize and train the model\n        net = LinearModel(dog_X_scaled_poly.shape[1], 3).to(device)\n\n        # Train the model\n        train_poly_loss, losses = train(net, 0.005, dog_X_scaled_poly, dog_Y, 1500)\n        training_losses.append(losses)\n\n        # Generate reconstructed image with no gradient tracking\n        with torch.no_grad():\n            output = net(dog_X_scaled_poly)\n\n        # Reshape output and apply transformations\n        reconstructed_image = output.cpu().reshape(crop.shape[1], crop.shape[2], -1)\n        clipped_image = clip(reconstructed_image)\n\n        # Append images to lists with correct shape\n        original_images.append(rearrange(crop, 'c h w -> h w c').cpu().numpy())\n        reconstructed_clipped_images.append(clipped_image.numpy())\n\n        # Explicitly delete unused tensors and clear cache\n        del dog_X_scaled_poly, output, reconstructed_image, clipped_image\n        torch.cuda.empty_cache()\n\n    print(\"Original Image Shape:\",original_images[0].shape)\n    print(\"Reconstructed Image Shape:\",reconstructed_clipped_images[0].shape)\n    print(reconstructed_clipped_images[0][0, :5, :])\n\n# Run the experiment\nrun()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a figure with subplots for loss and images\nfig, axs = plt.subplots(len(degrees),3, figsize=(10, len(degrees) * 3))\n\nlatexify()\n\n\nfor i, degree in enumerate(degrees):\n    # Plot training loss\n    axs[i, 0].plot(training_losses[i])\n    axs[i, 0].set_title(f\"Training Loss for Degree {degree}\")\n    axs[i, 0].set_xlabel(\"Epoch\")\n    axs[i, 0].set_ylabel(\"Loss\")\n    format_axes(axs[i, 0])\n\n    # Plot original image\n    axs[i, 1].imshow(original_images[0])\n    axs[i, 1].set_title(f\"Original Image\")\n    axs[i, 1].axis('off')\n\n    # Plot reconstructed clipped image\n    axs[i, 2].imshow(reconstructed_clipped_images[i])\n    axs[i, 2].set_title(f\"Reconstructed Clipped Image (Degree {degree})\")\n    axs[i, 2].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n# Plot all training losses in a single plot\nplt.figure(figsize=(12, 6))\nfor i, degree in enumerate(degrees):\n    plt.plot(training_losses[i], label=f'Degree {degree}')\nplt.title(\"Training Loss for Different Polynomial Degrees\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","metadata":{"id":"fuTl3hXx6Mxw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Reconstructing using Random Fourier Features","metadata":{"id":"7JWbm_DaZjuq"}},{"cell_type":"code","source":"import torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.cuda.amp import autocast, GradScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.kernel_approximation import RBFSampler\n\n# Clear unused memory after each iteration\ndef clear_memory():\n    torch.cuda.empty_cache()\n\n# Function to create RFF features\ndef create_rff_features(X, num_features, sigma):\n    rff = RBFSampler(n_components=num_features, gamma=1/(2 * sigma**2))\n    X = X.cpu().numpy()\n    X = rff.fit_transform(X)\n    return torch.tensor(X, dtype=torch.float32).to(device)\n\n\n# Training function with mixed precision\ndef train(net, learning_rate, X_data, Y_data, epochs, batch_size=256):\n    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n    criterion = nn.MSELoss()\n    scaler = GradScaler()  # Mixed precision scaler\n\n    # Create DataLoader for batch processing\n    dataset = TensorDataset(X_data, Y_data)\n    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    losses = []\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for batch_X, batch_Y in data_loader:\n            optimizer.zero_grad()\n            \n            # Forward pass with mixed precision\n            with autocast():\n                output = net(batch_X)\n                loss = criterion(output, batch_Y)\n            \n            # Backward pass and update gradients\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            epoch_loss += loss.item()\n\n        losses.append(epoch_loss / len(data_loader))\n\n        if epoch % 100 == 0:\n            print(f\"Epoch {epoch} loss: {epoch_loss / len(data_loader)}\")\n\n    return losses\n\n# Function to plot reconstructed and original images\ndef plot_reconstructed_and_original_image(crop, net, X_data, title):\n    with torch.no_grad():\n        reconstructed_image = net(X_data).cpu().numpy().reshape(crop.shape[1], crop.shape[2], -1)\n    \n    plt.figure(figsize=(10, 5))\n    plt.imshow(reconstructed_image)\n    plt.title(title)\n    plt.axis('off')\n    plt.show()\n\n# Function to train and return the loss and the trained network\ndef train_model(X_data, features, sigma, learning_rate=0.005, epochs=2500):\n    # Generate random Fourier features\n    X_rff = create_rff_features(X_data, features, sigma)\n    \n    # Initialize and train the model\n    net = LinearModel(X_rff.shape[1], 3)\n    \n    # If using multiple GPUs\n    if torch.cuda.device_count() > 1:\n        net = nn.DataParallel(net)\n\n    net.to(device)\n    train_rff_losses = train(net, learning_rate, X_rff, dog_Y, epochs)\n    \n    return net, train_rff_losses, X_rff\n\n# Function to plot the training loss\ndef plot_training_loss(losses, features, sigma):\n    plt.figure(figsize=(10, 5))\n    plt.plot(losses)\n    plt.title(f\"Training Loss (Features: {features}, Sigma: {sigma})\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.show()\n\n# Function to plot the reconstructed image\ndef plot_reconstructed_image(crop, net, X_rff, features, sigma):\n    plot_reconstructed_and_original_image(crop, net, X_rff, title=f\"Reconstructed Image (Features: {features}, Sigma: {sigma})\")\n\n# Main function to test multiple combinations of features and sigma\ndef test_feature_sigma_combinations(X_data, crop, combinations, learning_rate=0.005, epochs=2500):\n    for comb in combinations:\n        features = comb[\"features\"]\n        sigma = comb[\"sigma\"]\n        \n        # Train the model and get losses\n        net, train_rff_losses, X_rff = train_model(X_data, features, sigma, learning_rate, epochs)\n        \n        # Plot the training loss\n        plot_training_loss(train_rff_losses, features, sigma)\n        \n        # Plot the reconstructed image\n        plot_reconstructed_image(crop, net, X_rff, features, sigma)\n        \n        # Clear memory after each iteration\n        clear_memory()\n\n# Define different feature and sigma combinations\nfeature_sigma_combinations = [\n    {\"features\": 5000, \"sigma\": 0.05},\n    {\"features\": 5000, \"sigma\": 0.1},\n    {\"features\": 15000, \"sigma\": 0.05},\n    {\"features\": 15000, \"sigma\": 0.1},\n    {\"features\": 37500, \"sigma\": 0.008},\n]\n\n# Run the test for the defined combinations\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Assuming X_data and crop are already defined\ntest_feature_sigma_combinations(X_data, crop, feature_sigma_combinations)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}