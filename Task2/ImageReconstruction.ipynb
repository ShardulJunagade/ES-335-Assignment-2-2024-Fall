{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"V28"},"accelerator":"TPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9392721,"sourceType":"datasetVersion","datasetId":5700164}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Image Reconstruction\n\nChoose any image you like. Use Random Fourier Features (RFF) and Linear Regression to learn the mapping from the image coordinates (X, Y) to the pixel colors (R, G, B). Here, (X, Y) represents the coordinates of the pixels, and (R, G, B) represents the color values at those coordinates.\n\n1. **Load Image**: Select any image of your choice.\n2. **Random Fourier Features (RFF)**: Implement RFF to map pixel coordinates to color values.\n3. **Linear Regression**: Use linear regression to learn the mapping.\n4. **Display Results**: Show both the original and reconstructed images.\n5. **Metrics**: Calculate the Root Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR) between the original and reconstructed images.\n\n**Key Variables**:\n- X, Y: Pixel coordinates.\n- R, G, B: Pixel color values.\n","metadata":{"id":"AomtBF2P2E2h"}},{"cell_type":"markdown","source":"#### Importing necessary libraries","metadata":{"id":"Ng6UtYTC2E2i"}},{"cell_type":"code","source":"import torch\nprint(torch.__version__)\n\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Remove all the warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set env CUDA_LAUNCH_BLOCKING=1\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\ntry:\n    from einops import rearrange\nexcept ImportError:\n    %pip install einops\n\n    from einops import rearrange","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L-6NzlyK2E2i","outputId":"1f2a2c8f-1b83-4f35-c10f-51f50fb00444","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(torch.cuda.get_device_name(0))\nprint(\"CUDA available:\", torch.cuda.is_available())","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":335},"id":"QVq4V7Qo3bnm","outputId":"9719c186-4f52-4300-a6fd-bdf47433cc31","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Importing image","metadata":{"id":"tehY3M7g2E2i"}},{"cell_type":"code","source":"import os\n\n# Define the directory path\ndirectory_path = '../assets/images/'\n\n# Check if the directory exists, and create it if it does not\nif not os.path.exists(directory_path):\n    os.makedirs(directory_path)\n\n# Check if the file exists\nif os.path.exists(os.path.join(directory_path, 'dog.jpg')):\n    print('dog.jpg exists')\nelse:\n    # Download the file if it does not exist\n    !wget https://segment-anything.com/assets/gallery/AdobeStock_94274587_welsh_corgi_pembroke_CD.jpg -O ../assets/images/dog.jpg\n","metadata":{"id":"3uMUqUX32E2j","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read in a image from torchvision\nimg = torchvision.io.read_image(\"../assets/images/dog.jpg\")\nprint(img.shape)","metadata":{"id":"KfpLsvGv2E2j","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(rearrange(img, 'c h w -> h w c').numpy())\nplt.axis('off')\nplt.show()","metadata":{"id":"gg0V2vQS2E2j","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Read in a image from torchvision\n# iitgn_img = torchvision.io.read_image(\"../assets/images/iitgn.jpg\")\n# print(iitgn_img.shape)\n# plt.imshow(rearrange(iitgn_img, 'c h w -> h w c').numpy())\n","metadata":{"id":"xDbkONTy2E2j","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\n\nscaler_img = preprocessing.MinMaxScaler().fit(img.reshape(-1, 1))\nscaler_img","metadata":{"id":"KqF-dvdV2E2j","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimg_scaled = scaler_img.transform(img.reshape(-1, 1)).reshape(img.shape)\nprint(img_scaled.shape)\n\nimg_scaled = torch.tensor(img_scaled)\n","metadata":{"id":"Njg6Vs1j2E2k","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_scaled = img_scaled.to(device)\nimg_scaled","metadata":{"id":"1kAV6vp82E2k","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Crop the image","metadata":{"id":"G0yrB0-k2E2l"}},{"cell_type":"code","source":"crop = torchvision.transforms.functional.crop(img_scaled.cpu(), 600, 800, 300, 300)\ncrop.shape","metadata":{"id":"yeE4dqtz2E2l","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(rearrange(crop, 'c h w -> h w c').cpu().numpy())\nplt.axis('off')\nplt.show()","metadata":{"id":"kyT5Q7rY2E2l","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"crop = crop.to(device)","metadata":{"id":"SnTfz3ib2E2l","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Create a coordinate map","metadata":{"id":"CC0JpjIX2E2l"}},{"cell_type":"code","source":"def create_coordinate_map(img):\n    \"\"\"\n    img: torch.Tensor of shape (num_channels, height, width)\n\n    return: tuple of torch.Tensor of shape (height * width, 2) and torch.Tensor of shape (height * width, num_channels)\n    \"\"\"\n\n    num_channels, height, width = img.shape\n    print(\"Number of channels:\", num_channels, \"\\nHeight:\", height, \"\\nWidth:\", width)\n    # Create a 2D grid of (x,y) coordinates (h, w)\n    # width values change faster than height values\n    w_coords = torch.arange(width).repeat(height, 1)\n    h_coords = torch.arange(height).repeat(width, 1).t()\n    w_coords = w_coords.reshape(-1)\n    h_coords = h_coords.reshape(-1)\n\n    # Combine the x and y coordinates into a single tensor\n    X = torch.stack([h_coords, w_coords], dim=1).float()\n\n    # Move X to GPU if available\n    X = X.to(device)\n    print(\"X shape:\", X.shape)\n    # Reshape the image to (h * w, num_channels)\n    Y = rearrange(img, 'c h w -> (h w) c').float()\n    print(\"Y shape:\", Y.shape)\n\n    return X, Y","metadata":{"id":"1_4v6UGh2E2l","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dog_X, dog_Y = create_coordinate_map(crop)\n\nprint(dog_X) # (300*300, 2)- coordinates\nprint(dog_Y) # (300*300, 3)- RGB values","metadata":{"id":"l0-n7P6z2E2m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MinMaxScaler from -1 to 1\nscaler_X = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(dog_X.cpu())\n\n# Scale the X coordinates\ndog_X_scaled = scaler_X.transform(dog_X.cpu())\n\n# Move the scaled X coordinates to the GPU\ndog_X_scaled = torch.tensor(dog_X_scaled).to(device)\n\n# Set to dtype float32\ndog_X_scaled = dog_X_scaled.float()","metadata":{"id":"pzdHW9ME2E2m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Reconstructing using Linear Model","metadata":{"id":"wRrUUUMC2E2m"}},{"cell_type":"code","source":"torch.manual_seed(42) # Set seed for reproducibility\n\nclass LinearModel(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(LinearModel, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        return self.linear(x)\n","metadata":{"id":"XYAVdZur2E2m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = LinearModel(2, 3)\nnet.to(device)\nprint(net)\n","metadata":{"id":"z4jikg1R2E2m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Weights:\", net.linear.weight)\nprint(\"Bias:\", net.linear.bias)","metadata":{"id":"lruWBsPA2E2m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(net, lr, X, Y, epochs, verbose=True):\n    \"\"\"\n    net: torch.nn.Module\n    lr: float\n    X: torch.Tensor of shape (num_samples, 2)\n    Y: torch.Tensor of shape (num_samples, 3)\n    \"\"\"\n    losses = []\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n    for epoch in range(1,epochs+1):\n        optimizer.zero_grad()\n        outputs = net(X)\n        loss = criterion(outputs, Y)\n        losses.append(loss.item())\n        loss.backward()\n        optimizer.step()\n        if verbose and epoch % 100 == 0:\n            print(f\"Epoch {epoch} loss: {loss.item():.6f}\")\n    return loss.item(),losses","metadata":{"id":"ALWXwzdz2E2m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss,training_losses = train(net, 0.01, dog_X_scaled, dog_Y, 1000)","metadata":{"id":"n0Wbsb2A2E2m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nCode snippet copied from [https://github.com/nipunbatra/ml-teaching/blob/e2cd59d3e3358473ebfcf70e71d70361bb4501b4/latexify.py#L9] by Nipun Batra\nDate: 14 th August , 2024\n\nChanges :\n'text.latex.preamble': '\\\\usepackage{gensymb}',\n'text.usetex': False,\n\n'''\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport matplotlib\n\nfrom math import sqrt\nSPINE_COLOR = 'gray'\n\ndef latexify(fig_width=None, fig_height=None, columns=1):\n    \"\"\"Set up matplotlib's RC params for LaTeX plotting.\n    Call this before plotting a figure.\n\n    Parameters\n    ----------\n    fig_width : float, optional, inches\n    fig_height : float,  optional, inches\n    columns : {1, 2}\n    \"\"\"\n\n    # code adapted from http://www.scipy.org/Cookbook/Matplotlib/LaTeX_Examples\n\n    # Width and max height in inches for IEEE journals taken from\n    # computer.org/cms/Computer.org/Journal%20templates/transactions_art_guide.pdf\n\n    assert(columns in [1,2])\n\n    if fig_width is None:\n        fig_width = 3.39 if columns==1 else 6.9 # width in inches\n\n    if fig_height is None:\n        golden_mean = (sqrt(5)-1.0)/2.0    # Aesthetic ratio\n        fig_height = fig_width*golden_mean # height in inches\n\n    MAX_HEIGHT_INCHES = 8.0\n    if fig_height > MAX_HEIGHT_INCHES:\n        print(\"WARNING: fig_height too large:\" + fig_height + \n              \"so will reduce to\" + MAX_HEIGHT_INCHES + \"inches.\")\n        fig_height = MAX_HEIGHT_INCHES\n\n    params = {'backend': 'ps',\n              'text.latex.preamble': '\\\\usepackage{gensymb}',\n              'axes.labelsize': 8, # fontsize for x and y labels (was 10)\n              'axes.titlesize': 8,\n              'font.size': 8, # was 10\n              'legend.fontsize': 8, # was 10\n              'xtick.labelsize': 8,\n              'ytick.labelsize': 8,\n              'text.usetex': False,\n              'figure.figsize': [fig_width,fig_height],\n              'font.family': 'serif'\n    }\n\n    matplotlib.rcParams.update(params)\n\n\ndef format_axes(ax):\n\n    for spine in ['top', 'right']:\n        ax.spines[spine].set_visible(False)\n\n    for spine in ['left', 'bottom']:\n        ax.spines[spine].set_color(SPINE_COLOR)\n        ax.spines[spine].set_linewidth(0.5)\n\n    ax.xaxis.set_ticks_position('bottom')\n    ax.yaxis.set_ticks_position('left')\n\n    for axis in [ax.xaxis, ax.yaxis]:\n        axis.set_tick_params(direction='out', color=SPINE_COLOR)\n\n    return ax\n\n","metadata":{"id":"tkgn7705_XnB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot training loss graph\nplt.figure(figsize=(10, 5))\nlatexify()\nformat_axes(plt.gca())\nplt.plot(training_losses)\nplt.title(\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.show()\n\n","metadata":{"id":"YPVfOlPl2E2m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_reconstructed_and_original_image(original_img, net, X, title=\"\"):\n    \"\"\"\n    net: torch.nn.Module\n    X: torch.Tensor of shape (num_samples, 2)\n    Y: torch.Tensor of shape (num_samples, 3)\n    \"\"\"\n    num_channels, height, width = original_img.shape\n    net.eval()\n    with torch.no_grad():\n        outputs = net(X)\n        outputs = outputs.reshape(height, width, num_channels)\n        #outputs = outputs.permute(1, 2, 0)\n    fig = plt.figure(figsize=(6, 4))\n    gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])\n\n    ax0 = plt.subplot(gs[0])\n    ax1 = plt.subplot(gs[1])\n\n    ax0.imshow(outputs.cpu())\n    ax0.set_title(\"Reconstructed Image\")\n\n\n    ax1.imshow(original_img.cpu().permute(1, 2, 0))\n    ax1.set_title(\"Original Image\")\n\n    for a in [ax0, ax1]:\n        a.axis(\"off\")\n\n\n    fig.suptitle(title, y=0.9)\n    plt.tight_layout()","metadata":{"id":"1GBnrJ472E2m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_reconstructed_and_original_image(crop, net, dog_X_scaled, title=\"Reconstructed Image\")","metadata":{"id":"6Kau8IBw2E2n","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Using Polynomial Basis Functions","metadata":{"id":"A4cdYYz72E2n"}},{"cell_type":"code","source":"# Use polynomial features of degree \"d\"\n\ndef poly_features(X, degree):\n    \"\"\"\n    X: torch.Tensor of shape (num_samples, 2)\n    degree: int\n\n    return: torch.Tensor of shape (num_samples, degree * (degree + 1) / 2)\n    \"\"\"\n    X1 = X[:, 0]\n    X2 = X[:, 1]\n    X1 = X1.unsqueeze(1)\n    X2 = X2.unsqueeze(1)\n    X = torch.cat([X1, X2], dim=1)\n    poly = preprocessing.PolynomialFeatures(degree=degree)\n    X = poly.fit_transform(X.cpu())\n    return torch.tensor(X, dtype=torch.float32).to(device)","metadata":{"id":"OAfs68wg2E2n","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def normalize_image(image):\n    \"\"\"Normalize image to [0, 1] range for float images.\"\"\"\n    image_min = image.min()\n    image_max = image.max()\n    return (image - image_min) / (image_max - image_min)\n\ndef sigmoid(x):\n    return 1 / (1 + torch.exp(-x))\n\ndef apply_sigmoid(image_tensor):\n    \"\"\"Apply sigmoid to image tensor to normalize it to [0, 1] range.\"\"\"\n    return sigmoid(image_tensor)\n\ndef clip(image_tensor):\n    \"\"\"Clip image tensor to [0, 1] range.\"\"\"\n    return torch.clamp(image_tensor, 0, 1)\n\n","metadata":{"id":"U0-TjSDo799p","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define polynomial degrees to test\ndegrees = [5, 10, 50, 100]\n\n# Initialize lists to store training losses and images for each degree\ntraining_losses = []\noriginal_images = []\nreconstructed_clipped_images = []\n\nfor degree in degrees:\n    # Create polynomial features\n    dog_X_scaled_poly = poly_features(dog_X_scaled, degree)\n    print(f\"Degree {degree}: {dog_X_scaled_poly.dtype}, {dog_X_scaled_poly.shape}, {dog_Y.shape}, {dog_Y.dtype}\")\n\n    # Initialize and train the model\n    net = LinearModel(dog_X_scaled_poly.shape[1], 3)\n    net.to(device)\n\n    # Train the model\n    train_poly_loss, losses = train(net, 0.005, dog_X_scaled_poly, dog_Y, 1500)\n    training_losses.append(losses)\n\n    # Generate reconstructed image\n    with torch.no_grad():\n        output = net(dog_X_scaled_poly)\n\n    # Reshape output and apply transformations\n    reconstructed_image = output.cpu().reshape(crop.shape[1], crop.shape[2], -1)\n    clipped_image = clip(reconstructed_image)\n\n    # Append images to lists with correct shape\n    original_images.append(rearrange(crop, 'c h w -> h w c').cpu().numpy())\n    reconstructed_clipped_images.append(clipped_image.numpy())","metadata":{"id":"9iyp7tyk2E2n","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"XVAYdeiJZir-"}},{"cell_type":"code","source":"print(original_images[0].shape)\nprint(reconstructed_clipped_images[0].shape)","metadata":{"id":"TIE2e0zW_-mw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(reconstructed_clipped_images[0][0,:5,:])","metadata":{"id":"-XrpUWWgGRQK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a figure with subplots for loss and images\nfig, axs = plt.subplots(len(degrees),3, figsize=(10, len(degrees) * 3))\n\nlatexify()\n\n\nfor i, degree in enumerate(degrees):\n    # Plot training loss\n    axs[i, 0].plot(training_losses[i])\n    axs[i, 0].set_title(f\"Training Loss for Degree {degree}\")\n    axs[i, 0].set_xlabel(\"Epoch\")\n    axs[i, 0].set_ylabel(\"Loss\")\n    format_axes(axs[i, 0])\n\n    # Plot original image\n    axs[i, 1].imshow(original_images[0])\n    axs[i, 1].set_title(f\"Original Image\")\n    axs[i, 1].axis('off')\n\n    # Plot reconstructed clipped image\n    axs[i, 2].imshow(reconstructed_clipped_images[i])\n    axs[i, 2].set_title(f\"Reconstructed Clipped Image (Degree {degree})\")\n    axs[i, 2].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"fuTl3hXx6Mxw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot all training losses in a single plot\nplt.figure(figsize=(12, 6))\nfor i, degree in enumerate(degrees):\n    plt.plot(training_losses[i], label=f'Degree {degree}')\nplt.title(\"Training Loss for Different Polynomial Degrees\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","metadata":{"id":"T-6zd4it8_3f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Reconstructing using Random Fourier Features","metadata":{"id":"7JWbm_DaZjuq"}},{"cell_type":"code","source":"# create RFF features\ndef create_rff_features(X, num_features, sigma):\n    from sklearn.kernel_approximation import RBFSampler\n    rff = RBFSampler(n_components=num_features, gamma=1/(2 * sigma**2))\n    X = X.cpu().numpy()\n    X = rff.fit_transform(X)\n    return torch.tensor(X, dtype=torch.float32).to(device)\n","metadata":{"id":"BnItZWyRZpbY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_features = 37500\nsigma = 0.008\nX_rff = create_rff_features(dog_X_scaled, n_features, sigma)\nprint(X_rff.shape)","metadata":{"id":"-CE4s42UZnEr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = LinearModel(X_rff.shape[1], 3)\n# Move model to GPUs if available\nif torch.cuda.device_count() > 1:\n    net = nn.DataParallel(net)  # Wrap model to use multiple GPUs\nnet.to(device)\n\ntrain_rff_loss , train_rff_losses = train(net, 0.005, X_rff, dog_Y, 2500)","metadata":{"id":"OFsbK_nDaMGc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot training loss graph\nplt.figure(figsize=(10, 5))\nlatexify()\nformat_axes(plt.gca())\nplt.plot(train_rff_losses)\nplt.title(\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.show()\n\n","metadata":{"id":"x6vTadLYaYOb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_reconstructed_and_original_image(crop, net, X_rff, title=\"Reconstructed Image with RFF Features\")","metadata":{"id":"Js3E6x-hZ-Jz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a function to train the model and return the loss and the trained network\ndef train_model(X_data, features, sigma, learning_rate=0.005, epochs=2500):\n    # Generate random Fourier features\n    X_rff = generate_rff(X_data, features, sigma)\n    \n    # Initialize and train the model\n    net = LinearModel(X_rff.shape[1], 3)\n    net.to(device)\n    train_rff_loss, train_rff_losses = train(net, learning_rate, X_rff, dog_Y, epochs)\n    \n    return net, train_rff_losses, X_rff\n\n# Define a function to plot the training loss\ndef plot_training_loss(losses, features, sigma):\n    plt.figure(figsize=(10, 5))\n    latexify()\n    format_axes(plt.gca())\n    plt.plot(losses)\n    plt.title(f\"Training Loss (Features: {features}, Sigma: {sigma})\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.show()\n\n# Define a function to plot the reconstructed image\ndef plot_reconstructed_image(crop, net, X_rff, features, sigma):\n    plot_reconstructed_and_original_image(crop, net, X_rff, title=f\"Reconstructed Image (Features: {features}, Sigma: {sigma})\")\n\n# Main function to test multiple combinations\ndef test_feature_sigma_combinations(X_data, crop, combinations, learning_rate=0.005, epochs=2500):\n    for comb in combinations:\n        features = comb[\"features\"]\n        sigma = comb[\"sigma\"]\n        \n        # Train the model and get losses\n        net, train_rff_losses, X_rff = train_model(X_data, features, sigma, learning_rate, epochs)\n        \n        # Plot the training loss\n        plot_training_loss(train_rff_losses, features, sigma)\n        \n        # Plot the reconstructed image\n        plot_reconstructed_image(crop, net, X_rff, features, sigma)\n\n# Define different feature and sigma combinations\nfeature_sigma_combinations = [\n    {\"features\": 5000, \"sigma\": 0.05},\n    {\"features\": 5000, \"sigma\": 0.1},\n    {\"features\": 15000, \"sigma\": 0.05},\n    {\"features\": 15000, \"sigma\": 0.1},\n]\n\n# Run the test for the defined combinations\ntest_feature_sigma_combinations(X_data, crop, feature_sigma_combinations)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}